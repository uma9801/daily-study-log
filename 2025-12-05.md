#SQL #python
# データベース
プライマリキー(PK)
列：カラム
行：レコード
データベース→テーブル→データ
## SQL
Structured Query Language
データベースにクエリを投げて、データを得る。
`SELECT *orカラム`　カラム指定。`*`で全選択になる。
`FROM テーブル`　テーブル指定
`WHERE カラム = 文字列or値`　指定抽出
データベースのデータ型
- テキストデータ
- 数値データ
- 日付データ
`WHERE 条件式`で条件付き検索可能。日付にも比較演算子が適用できる。
- `WHERE purchesed <= '2025-12-05'`
`LIKE`演算子
- ～のような。転じて〇〇を含むレコードを取得する。
- `WHERE カラム = '文字列'`を`WHERE カラム LIKE '文字列'`にしてしまわないよう注意。
ワイルドカード：どんな文字列にも一致することを指す記号
- LIKE演算子では`%`
- 例：`WHERE name LIKE '%抹茶%'`
- '抹茶'を含むデータを全て取得。
- `'抹茶%'`なら前方一致。`'%抹茶'`なら後方一致。
# 理解不足なものを視覚化
- Seleniumでwebスクレイピングしてデータベースで管理運用する
- 業務フローのイメージ
	- データベースを設ける
	- Seleniumでスクレイピングしてデータを得る
	- 得られたデータを適切な形に整える
	- 整形したデータをデータベースに保存
	- pandasを使ってデータを適宜加工
	- 加工したデータを出力する
	- 必要があれば加工したデータを保存する
- Seleniumが分かってない、使えない
	- スクレイピングの方法
	- データの受け渡しはどうする？
- データベースの大枠は分かる
	- どんな仕組み？
	- どう扱う？
	- 管理は？
- 得られたデータを実際にpandasでどう扱う？
	- 加工したデータをどうする？
	- 出力方法は？保存は？
	- どういう形で保存するのが適切？
## 勉強テーマ
- データベース
	- 実態がよくわかってないので一から知る
	- 調査結果
		- 必要データベースとライブラリ
		- データベース本体：SQlite(sqlite3)
			- SQLiteはPythonの標準ライブラリに含まれているため、`pip install` なしで即座に使用できる。
			- ファイルベースで動作し、サーバーの構築・管理が一切不要な点が研究環境に最適。
			- **メリット**:
			    - **セットアップが容易**: サーバーレスで、単一のファイルとしてデータベースが管理される。
			    - **移植性・再現性**: データベースファイル (`.db` ファイル) を Git で管理したり、共同研究者にメールで送ったりするだけで環境を共有できる。
			    - **十分な機能**: 経済学の研究データ（文献情報、スクレイピング結果、計量分析用データなど）を管理するには十分な機能と性能を備えている。
			- 一般的な接続・操作手順 (SQLiteの例)
				Pythonでデータベースを操作する際は、PEP 249で定義されているDB-API 2.0という標準仕様に準拠したライブラリを使用するのが一般的です。 
				以下は、`sqlite3` を使用した基本的な接続・操作の例です。
				1. **データベースへの接続**: `connect()` メソッドでデータベースファイルに接続し、接続オブジェクトを取得します。
				2. **カーソルの取得**: 接続オブジェクトの `cursor()` メソッドでカーソルオブジェクトを取得します。カーソルはSQLコマンドを実行し、結果を処理するために使用します。
				3. **SQLの実行**: カーソルオブジェクトの `execute()` メソッドでSQLクエリ（`CREATE TABLE`, `INSERT`, `SELECT` など）を実行します。
				4. **変更のコミット**: データの挿入や更新を行った場合は、接続オブジェクトの `commit()` メソッドで変更を保存します。
				5. **接続の終了**: `close()` メソッドで接続を閉じます。 
				詳細な手順や他のデータベースへの接続方法は、[Pythonの公式ドキュメント](https://docs.python.org/ja/3/library/sqlite3.html) や各種ライブラリのガイドを参照できます。
		- ライブラリ（操作レイヤー）：SQLAlchemy
			- `sqlite3` などのDB-APIに直接SQL文を記述しても操作は可能だが、データ収集（Selenium）や分析コードと統合する際には、SQLAlchemy のようなORM（オブジェクト関係マッピング）を使うとコードの見通しが格段に良くなる。
			- **メリット**:
			    - **Pythonネイティブな操作**: データベースのテーブルやレコードをPythonのクラスやオブジェクトとして扱えるため、Seleniumで取得したデータをPythonコード内でそのまま挿入できる。
			    - **データベース非依存**: 万が一、将来的にMySQLやPostgreSQLへ移行する必要が生じても、コードの大部分を書き直す必要がない。
			    - **データ構造の明確化**: 研究データのスキーマ（構造）をPythonのコードとして定義するため、どのようなデータがどのように保存されているかが明確になり、再現性が高まる。
		- データ収集（Selenium）との連携
			- Seleniumを使ってWebサイトからデータをスクレイピングする際も、SQLAlchemyと組み合わせることでスムーズなワークフローを構築できる。
				1. Seleniumでデータを抽出する。
				2. 抽出したデータをPythonのオブジェクト（SQLAlchemyで定義したモデルのインスタンス）に格納する。
				3. そのオブジェクトをデータベースセッションに追加し、コミットする。  
		- 構成の理由
			学術研究のデータベースは、商用システムのような24時間365日の高可用性や、秒間数千件のトランザクション処理性能を必ずしも必要としない。むしろ、研究室内での閉じられた環境での運用、データの再現性、セットアップとメンテナンスの手軽さが重要になる。
- Selenium
	- 環境構築
		- Dockerの勉強が要るかも
	- スクレイピング
	- データ出力・保存
- pandas使用におけるデータ出力と保存
# 人間関係を円滑にするために信頼を築く＆続く会話
3レイヤー
- 抽象化：5W1H＋価値観やイメージ
- 事実確認：5W1H（まるで仕事）
- 具体化：5W1H＋言動や感情
相手から感情や言動を引き出す質問ができれば会話が続く
事実確認になったとしても
- キーワードを1つだけピックアップして感情・言動に繋がる事を聞ければいい
キーワードピッキングが重要なポイント
- 何でもいいので相手の話から「1つの名詞」だけをピックアップして深掘りする
	- 何でもいいし正解はないが、例えば...
		- 興味（感情が動いた）を持った単語
		- 初めて聞いたorよく知らない単語
		- 具体的なイメージが頭に浮かんだ単語
相手の使う動詞に注目する
- ディズニーランドに行った
	- ディズニーランドに行ったことに価値を感じている
- ディズニーランドに行ってＢサンダーマウンテンに乗った
	- 乗ったことに価値を感じている
- 同じ場所に行っていても、興味の対象や大小は様々
	- 最後に使われた動詞から相手の興味の対象を理解する

動画では
- 具体化（具体的には？「車」」「飛行機」 ）：追体験
- 抽象化（こういうイメージかな？「乗り物」 ）：価値観
- 事実確認を繰り返す「横レイヤー（レイヤーが変わらない）移動」は
	- 相手の理解にもつながらない
	- 相手からの信頼にもつながらない
	- として避ける会話とされている
- 抽象化・具体化は相手の体験を追い、価値観を認識する
	- 相手を評価せず
	- 相手を認める・知る
	- 総じて相手を理解しようとする姿勢であり
	- ジャッジしない安心感を与え
	- 信頼を築いていける
- テクニックとしては
	- 具体化
		- キーワードから具体的に
		- したこと、思ったこと、感じたこと
		- エピソードを引き出す
		- キーワードを掘りつくしたら
			- 更にキーワードピッキングで別の話にスライド
		- 具体化を基に抽象化にもスライドする
	- 抽象化
		- 相手の話に対し
			- 「共感」＋実体験（自己開示）
			- 難易度は高いが有効な
				- 「要約」＋価値観確認（問いかけ）
				- 要約を間違えるのはマイナスになりかねないので
				- それって〇〇ということ？
				- つまり〇〇で合ってる？
					- など適宜イメージの補正をしていくと解像度が高まる